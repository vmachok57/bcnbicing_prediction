{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":93461,"databundleVersionId":11131302,"sourceType":"competition"},{"sourceId":11219832,"sourceType":"datasetVersion","datasetId":7006827}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport xgboost as xgb\nimport dask.array as da\nimport dask.dataframe as dd\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n#bikes data\nfolder_path = \"/kaggle/input/bicing/data_no2020/data_no2020 copy\"\ncsv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\ndf_list = [dd.read_csv(file, assume_missing=True, dtype={'last_reported': 'float64','station_id':'float64','num_docks_available':'float64','is_returning':'float64','ttl':'float64', 'status':'object'}) for file in csv_files]\ndf_datos_bikes = dd.concat(df_list, ignore_index=True)\n\n#stations info + external sources\nstations_info = pd.read_csv('/kaggle/input/bicing/stations.csv')\ncalendar = pd.read_csv('/kaggle/input/bicing/calendar.csv')\nmetro = pd.read_csv('/kaggle/input/bicing/metro_bcn.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:59:12.446049Z","iopub.execute_input":"2025-03-30T21:59:12.446419Z","iopub.status.idle":"2025-03-30T21:59:12.778887Z","shell.execute_reply.started":"2025-03-30T21:59:12.446388Z","shell.execute_reply":"2025-03-30T21:59:12.777834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from geopy.distance import geodesic\n\n# Function to calculate the Haversine distance (in meters)\ndef haversine(lat1, lon1, lat2, lon2):\n    return geodesic((lat1, lon1), (lat2, lon2)).meters\n\n\nstations_info['nearest_station_id'] = None\nstations_info['nearest_station_distance'] = None\nstations_info['stations_within_100m'] = None\nstations_info['stations_within_300m'] = None\nstations_info['stations_within_500m'] = None\n\n# Iterate over each station to calculate distance to other station\nfor idx, station in stations_info.iterrows():\n    nearest_station = None\n    nearest_distance = float('inf')\n    count_100m = 0\n    count_300m = 0\n    count_500m = 0\n    \n    for jdx, other_station in stations_info.iterrows():\n        if station['station_id'] != other_station['station_id']:\n            distance = haversine(station['lat'], station['lon'], \n                                 other_station['lat'], other_station['lon'])\n            if distance < nearest_distance:\n                nearest_station = other_station['station_id']\n                nearest_distance = distance\n            \n            if distance <= 100:\n                count_100m += 1\n            if distance <= 300:\n                count_300m += 1\n            if distance <= 500:\n                count_500m += 1\n    \n    # Add into Dataframe\n    stations_info.at[idx, 'nearest_station_id'] = nearest_station\n    stations_info.at[idx, 'nearest_station_distance'] = nearest_distance\n    stations_info.at[idx, 'stations_within_100m'] = count_100m\n    stations_info.at[idx, 'stations_within_300m'] = count_300m\n    stations_info.at[idx, 'stations_within_500m'] = count_500m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:59:27.634047Z","iopub.execute_input":"2025-03-30T21:59:27.634419Z","iopub.status.idle":"2025-03-30T22:00:44.826328Z","shell.execute_reply.started":"2025-03-30T21:59:27.634389Z","shell.execute_reply":"2025-03-30T22:00:44.825151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Haversine formula to calculate the distance in meters\ndef haversine(lon1, lat1, lon2, lat2):\n    # Convert degrees to radians\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    # Haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n\n    # Radius of the Earth in meters (mean radius)\n    R = 6371000\n    distance = R * c\n    return distance\n\n\n# Function to find nearest metro station for each bicycle station\ndef find_nearest_metro(stations_info, metro):\n    nearest_metro_ids = []\n    nearest_distances = []\n\n    for _, bike_station in stations_info.iterrows():\n        bike_lat = bike_station['lat']\n        bike_lon = bike_station['lon']\n        \n        # Calculate the distance to each metro station and find the minimum\n        distances = metro.apply(lambda row: haversine(bike_lon, bike_lat, row['longitude'], row['latitude']), axis=1)\n        \n        # Find the minimum distance and the corresponding metro station\n        nearest_metro_id = metro.loc[distances.idxmin(), 'name']\n        nearest_distance = distances.min()\n        \n        nearest_metro_ids.append(nearest_metro_id)\n        nearest_distances.append(nearest_distance)\n    \n    stations_info['nearest_metro'] = nearest_metro_ids\n    stations_info['distance_to_nearest_metro'] = nearest_distances\n\n    return stations_info\n\n# Apply the function\nbicycle_stations_metro = find_nearest_metro(stations_info, metro)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:00:44.827724Z","iopub.execute_input":"2025-03-30T22:00:44.828021Z","iopub.status.idle":"2025-03-30T22:00:47.363033Z","shell.execute_reply.started":"2025-03-30T22:00:44.827996Z","shell.execute_reply":"2025-03-30T22:00:47.361698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Transformamos en un formato de hora legible\ndf_datos_bikes[\"last_reported\"] = dd.to_datetime(df_datos_bikes[\"last_reported\"], unit='s')\n#Descomponemos el Datatime (last_reported)\ndf_datos_bikes[\"year\"] = df_datos_bikes[\"last_reported\"].dt.year\ndf_datos_bikes[\"month\"] = df_datos_bikes[\"last_reported\"].dt.month\ndf_datos_bikes[\"day\"] = df_datos_bikes[\"last_reported\"].dt.day\ndf_datos_bikes[\"hour\"] = df_datos_bikes[\"last_reported\"].dt.hour\ndf_datos_bikes.dropna(subset=['last_reported'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:00:47.365222Z","iopub.execute_input":"2025-03-30T22:00:47.365607Z","iopub.status.idle":"2025-03-30T22:00:47.422581Z","shell.execute_reply.started":"2025-03-30T22:00:47.365578Z","shell.execute_reply":"2025-03-30T22:00:47.421495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_final = df_datos_bikes.groupby(['station_id', 'year', 'month', 'day', 'hour'])[['num_docks_available','num_bikes_available']].median().round(4).reset_index()\n# Asegurar que 'result' es un DataFrame de Pandas\nif not isinstance(df_final, pd.DataFrame):\n    df_final = df_final.compute() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:00:47.423938Z","iopub.execute_input":"2025-03-30T22:00:47.424316Z","iopub.status.idle":"2025-03-30T22:04:25.227452Z","shell.execute_reply.started":"2025-03-30T22:00:47.424279Z","shell.execute_reply":"2025-03-30T22:04:25.226511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configura la visualización de pandas para mostrar números con 4 decimales\npd.options.display.float_format = '{:,.4f}'.format\ndf_final = df_final.round(4)\n\ndf_final = df_final.loc[df_final['year'] != 1970]\ndf_final[['station_id','year','month','day','hour']] = df_final[['station_id','year','month','day','hour']].astype(int)\ndf_final.year.unique()\n\n# Unir los DataFrames usando 'station_id' como clave\nmerged_df = pd.merge(df_final, stations_info, on=\"station_id\", how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:04:25.228733Z","iopub.execute_input":"2025-03-30T22:04:25.229047Z","iopub.status.idle":"2025-03-30T22:04:43.376650Z","shell.execute_reply.started":"2025-03-30T22:04:25.229022Z","shell.execute_reply":"2025-03-30T22:04:43.375706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tenemos que cambiar capacity al max capacity de la estacion para que nos de como mucho 100% disponibilidad\nmerged_df['capacity'].fillna(merged_df['num_bikes_available'] + merged_df['num_docks_available'], inplace=True)\n\nmerged_df[\"num_docks_available_mod\"] = merged_df['capacity'] - merged_df['num_bikes_available']\n\n# Agregar la columna \"percentage_docks_available\"\nmerged_df[\"percentage_docks_available\"] = merged_df[\"num_docks_available_mod\"] / merged_df[\"capacity\"]\n\n# Opcional: llenar valores NaN si hay estaciones sin capacidad registrada\nmerged_df[\"percentage_docks_available\"].fillna(0, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:04:43.377671Z","iopub.execute_input":"2025-03-30T22:04:43.378034Z","iopub.status.idle":"2025-03-30T22:04:43.614948Z","shell.execute_reply.started":"2025-03-30T22:04:43.378001Z","shell.execute_reply":"2025-03-30T22:04:43.614046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ordenar los datos correctamente antes de agrupar\ndf_prediction_task = merged_df.sort_values(\n    by=['station_id', 'year', 'month', 'day', 'hour']\n)\n\nfor i in range(1, 5):\n    df_prediction_task = df_prediction_task.sort_values(by=['station_id','year', 'month', 'day', 'hour'])  # Ordena correctamente\n    df_prediction_task[f'disponibilidad_porcentage_{i}h_antes'] = (\n        df_prediction_task.groupby('station_id')['percentage_docks_available']\n        .shift(i)  # Desplazamiento de i horas\n        .fillna(0)  # Reemplaza NaN con 0\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:06:29.551781Z","iopub.execute_input":"2025-03-30T22:06:29.552312Z","iopub.status.idle":"2025-03-30T22:07:05.646143Z","shell.execute_reply.started":"2025-03-30T22:06:29.552270Z","shell.execute_reply":"2025-03-30T22:07:05.645049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ordenar cronológicamente y mostrar solo las columnas importantes\n#df_prediction_task = df_prediction_task.sort_values(by=['year', 'month', 'day', 'hour'])\n\ndf_prediction_task[['station_id', 'year', 'month', 'day', 'hour', \n                   'percentage_docks_available', \n                   'disponibilidad_porcentage_1h_antes', \n                   'disponibilidad_porcentage_2h_antes', \n                   'disponibilidad_porcentage_3h_antes', \n                   'disponibilidad_porcentage_4h_antes']].head(10)  # Puedes ajustar el número de filas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:07:05.647417Z","iopub.execute_input":"2025-03-30T22:07:05.647832Z","iopub.status.idle":"2025-03-30T22:07:05.977250Z","shell.execute_reply.started":"2025-03-30T22:07:05.647795Z","shell.execute_reply":"2025-03-30T22:07:05.976057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Only getting each 5th row to avoid having repeated data\ndf_prediction_sorted = df_prediction_task.sort_values(by=['station_id', 'year', 'month', 'day', 'hour']).reset_index(drop=True)\ndf_prediction_task = df_prediction_sorted.iloc[::5]\n\npercentil_1 = df_prediction_task['nearest_station_distance'].quantile(0.01)\npercentil_99 = df_prediction_task['nearest_station_distance'].quantile(0.99)\ndf_prediction_task['nearest_station_distance_cap']=df_prediction_task['nearest_station_distance'].clip(lower=percentil_1, upper=percentil_99)\n\ndf_prediction_task['Date'] = pd.to_datetime(df_prediction_task[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1))\ncalendar['Date'] = pd.to_datetime(calendar['date'], errors='coerce')\n\ndf_prediction_task = df_prediction_task.merge(\n    calendar[['Date','special_event']],  # Seleccionamos la columna específica de df_extra\n    on='Date',  # Columna común para la fusión\n    how='left'  # Realizamos el left join\n)\n\ndf_prediction_task['special_event'] = df_prediction_task['special_event'].replace({True: 1, False: 0})\n\n# Agregar la columna 'is_weekend' (1 si es sábado o domingo, 0 en otros casos)\ndf_prediction_task['is_weekend'] = df_prediction_task['Date'].dt.weekday.isin([5, 6]).astype(int)\n\nfestivos_data = [\n    (2022, 1, 1, \"Año Nuevo\"),\n    (2022, 1, 6, \"Reyes Magos\"),\n    (2022, 4, 15, \"Viernes Santo\"),\n    (2022, 4, 18, \"Lunes de Pascua\"),\n    (2022, 5, 1, \"Día del Trabajador\"),\n    (2022, 6, 6, \"Segunda Pascua (festivo local)\"),\n    (2022, 6, 24, \"San Juan\"),\n    (2022, 8, 15, \"La Asunción\"),\n    (2022, 9, 24, \"La Mercè (festivo local)\"),\n    (2022, 10, 12, \"Fiesta Nacional de España\"),\n    (2022, 11, 1, \"Todos los Santos\"),\n    (2022, 12, 6, \"Día de la Constitución\"),\n    (2022, 12, 8, \"La Inmaculada\"),\n    (2022, 12, 25, \"Navidad\"),\n    (2022, 12, 26, \"San Esteban\"),\n\n    (2023, 1, 1, \"Año Nuevo\"),\n    (2023, 1, 6, \"Reyes Magos\"),\n    (2023, 4, 7, \"Viernes Santo\"),\n    (2023, 4, 10, \"Lunes de Pascua\"),\n    (2023, 5, 1, \"Día del Trabajador\"),\n    (2023, 5, 29, \"Segunda Pascua (festivo local)\"),\n    (2023, 6, 24, \"San Juan\"),\n    (2023, 8, 15, \"La Asunción\"),\n    (2023, 9, 11, \"Diada de Cataluña\"),\n    (2023, 9, 25, \"La Mercè (festivo local)\"),\n    (2023, 10, 12, \"Fiesta Nacional de España\"),\n    (2023, 11, 1, \"Todos los Santos\"),\n    (2023, 12, 6, \"Día de la Constitución\"),\n    (2023, 12, 8, \"La Inmaculada\"),\n    (2023, 12, 25, \"Navidad\"),\n    (2023, 12, 26, \"San Esteban\"),\n\n    (2024, 1, 1, \"Año Nuevo\"),\n    (2024, 1, 6, \"Reyes Magos\"),\n    (2024, 3, 29, \"Viernes Santo\"),\n    (2024, 4, 1, \"Lunes de Pascua\"),\n    (2024, 5, 1, \"Día del Trabajador\"),\n    (2024, 5, 20, \"Segunda Pascua (festivo local)\"),\n    (2024, 6, 24, \"San Juan\"),\n    (2024, 8, 15, \"La Asunción\"),\n    (2024, 9, 11, \"Diada de Cataluña\"),\n    (2024, 9, 24, \"La Mercè (festivo local)\"),\n    (2024, 10, 12, \"Fiesta Nacional de España\"),\n    (2024, 11, 1, \"Todos los Santos\"),\n    (2024, 12, 6, \"Día de la Constitución\"),\n    (2024, 12, 8, \"La Inmaculada\"),\n    (2024, 12, 25, \"Navidad\"),\n    (2024, 12, 26, \"San Esteban\"),\n]\n\n# Crear el DataFrame\ndf_holidays = pd.DataFrame(festivos_data, columns=[\"year\", \"month\", \"day\", \"name\"])\n\n# Crear una nueva columna con la fecha completa en df_festivos\ndf_holidays[\"Date\"] = pd.to_datetime(df_holidays[[\"year\", \"month\", \"day\"]])\n\n# Marcar si la fecha en df_prediction_task es un feriado\ndf_prediction_task[\"is_holiday\"] = df_prediction_task[\"Date\"].isin(df_holidays[\"Date\"]).astype(int)\n# Filling N/As with Most Repeated Value\ndf_prediction_task.fillna(df_prediction_task.mode().iloc[0], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:07:05.979377Z","iopub.execute_input":"2025-03-30T22:07:05.979687Z","iopub.status.idle":"2025-03-30T22:07:40.256126Z","shell.execute_reply.started":"2025-03-30T22:07:05.979662Z","shell.execute_reply":"2025-03-30T22:07:40.255089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_prediction_task.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:07:40.258407Z","iopub.execute_input":"2025-03-30T22:07:40.258742Z","iopub.status.idle":"2025-03-30T22:07:40.265327Z","shell.execute_reply.started":"2025-03-30T22:07:40.258717Z","shell.execute_reply":"2025-03-30T22:07:40.264197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data_read = pd.read_csv('/kaggle/input/2025-bike-availability-prediction/metadata_sample_submission_2025.csv')\ntest_data = pd.merge(test_data_read, stations_info, on=\"station_id\", how=\"left\")\n\ntest_data['nearest_station_distance_cap']=test_data['nearest_station_distance'].clip(lower=percentil_1, upper=percentil_99)\ntest_data['Date'] = pd.to_datetime('2025-' + test_data['month'].astype(str).str.zfill(2) + '-' + test_data['day'].astype(str).str.zfill(2))\ncalendar['Date'] = pd.to_datetime(calendar['date'], errors='coerce')\n\ntest_data = test_data.merge(\n    calendar[['Date','special_event']],  # Seleccionamos la columna específica de df_extra\n    on='Date',  # Columna común para la fusión\n    how='left'  # Realizamos el left join\n)\n\ntest_data['special_event'] = test_data['special_event'].replace({True: 1, False: 0})\n\n# Agregar la columna 'is_weekend' (1 si es sábado o domingo, 0 en otros casos)\ntest_data['is_weekend'] = test_data['Date'].dt.weekday.isin([5, 6]).astype(int)\n\n# Crear una nueva columna con la fecha completa en df_festivos\ndf_holidays[\"Date\"] = pd.to_datetime(df_holidays[[\"year\", \"month\", \"day\"]])\n\n# Marcar si la fecha en df_prediction_task es un feriado\ntest_data[\"is_holiday\"] = test_data[\"Date\"].isin(df_holidays[\"Date\"]).astype(int)\n# Filling N/As with Most Repeated Value\ntest_data.fillna(df_prediction_task.mode().iloc[0], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:33:33.327864Z","iopub.execute_input":"2025-03-30T22:33:33.328366Z","iopub.status.idle":"2025-03-30T22:33:36.911454Z","shell.execute_reply.started":"2025-03-30T22:33:33.328326Z","shell.execute_reply":"2025-03-30T22:33:36.910445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = test_data.rename(columns={\n    'ctx-1': 'disponibilidad_porcentage_1h_antes',\n    'ctx-2': 'disponibilidad_porcentage_2h_antes',\n    'ctx-3': 'disponibilidad_porcentage_3h_antes',\n    'ctx-4': 'disponibilidad_porcentage_4h_antes'\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:33:36.913096Z","iopub.execute_input":"2025-03-30T22:33:36.913380Z","iopub.status.idle":"2025-03-30T22:33:37.054167Z","shell.execute_reply.started":"2025-03-30T22:33:36.913355Z","shell.execute_reply":"2025-03-30T22:33:37.053043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:33:37.055887Z","iopub.execute_input":"2025-03-30T22:33:37.056186Z","iopub.status.idle":"2025-03-30T22:33:37.063047Z","shell.execute_reply.started":"2025-03-30T22:33:37.056161Z","shell.execute_reply":"2025-03-30T22:33:37.061757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Define feature columns and target\nfeatures = ['month', 'hour', 'capacity', 'is_weekend', 'is_holiday',\n           'disponibilidad_porcentage_1h_antes','altitude', #'special_event',\n          'disponibilidad_porcentage_2h_antes', \n          # 'disponibilidad_porcentage_3h_antes',\n           #'disponibilidad_porcentage_4h_antes', \n            'nearest_station_distance_cap',\n            'stations_within_300m']\n\ntarget_variable = 'percentage_docks_available'\n\n# Drop NA rows in training data\ndf_prediction_task = df_prediction_task.dropna(subset=features + [target_variable])\n\n# Split into features and target\nX_train = df_prediction_task[features]\ny_train = df_prediction_task[target_variable]\n\n# Standardize the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Initialize and train model\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\nxgb_model.fit(X_train_scaled, y_train)\n\n# --- APPLY TO TEST DATA ---\n# Ensure test data has necessary features (and drop NA if needed)\ntest_data = test_data.dropna(subset=features)\nX_test = test_data[features]\nX_test_scaled = scaler.transform(X_test)  # Use the same scaler from training\n\n# Make predictions\ntest_data['percentage_docks_available'] = xgb_model.predict(X_test_scaled)\n\n# --- OPTIONAL: Evaluate if target is available in test_data ---\nif target_variable in test_data.columns:\n    y_test = test_data[target_variable]\n    y_pred = test_data['percentage_docks_available']\n    r2 = r2_score(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    print(f\"Test R-squared: {r2:.4f}\")\n    print(f\"Test Mean Squared Error: {mse:.4f}\")\n\n# --- Feature Importance ---\nplt.figure(figsize=(10, 8))\nxgb.plot_importance(xgb_model, importance_type='weight', max_num_features=10,\n                    title='XGBoost Feature Importance', height=0.8,\n                    xlabel='Weight (number of times the feature is used)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:33:40.099659Z","iopub.execute_input":"2025-03-30T22:33:40.100011Z","iopub.status.idle":"2025-03-30T22:33:52.350712Z","shell.execute_reply.started":"2025-03-30T22:33:40.099986Z","shell.execute_reply":"2025-03-30T22:33:52.349638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data[['percentage_docks_available']].reset_index().to_csv('submission_new.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:33:52.352147Z","iopub.execute_input":"2025-03-30T22:33:52.352445Z","iopub.status.idle":"2025-03-30T22:33:52.950420Z","shell.execute_reply.started":"2025-03-30T22:33:52.352414Z","shell.execute_reply":"2025-03-30T22:33:52.949266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/working/submission_new.csv')\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:33:54.740394Z","iopub.execute_input":"2025-03-30T22:33:54.740797Z","iopub.status.idle":"2025-03-30T22:33:54.834880Z","shell.execute_reply.started":"2025-03-30T22:33:54.740765Z","shell.execute_reply":"2025-03-30T22:33:54.833855Z"}},"outputs":[],"execution_count":null}]}